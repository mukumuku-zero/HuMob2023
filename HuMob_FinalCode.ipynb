{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_hIyICF0OxZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "import geobleu\n",
        "\n",
        "# 警告を無視する設定\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "def count_data_within_m_hours(uid, d_t_combined, m):\n",
        "    # uidとd_t_combinedを考慮したデータの抽出\n",
        "    selected_data = data[(data['uid'] == uid) & (data['d_t_combined'] <= d_t_combined)]\n",
        "\n",
        "    # 過去m時間までのデータ数をカウント\n",
        "    count = selected_data[(d_t_combined - selected_data['d_t_combined']) <= m].shape[0]\n",
        "\n",
        "    return count\n",
        "\n",
        "def featurecreation(data, pcnum, bcnum):\n",
        "\n",
        "    # 曜日をsin, cosを用いて表現する特徴量を追加\n",
        "    data['d_sin'] = np.sin(2 * np.pi * data['d'] / 7)\n",
        "    data['d_cos'] = np.cos(2 * np.pi * data['d'] / 7)\n",
        "\n",
        "    # 時間をsin, cosを用いて表現する特徴量を追加\n",
        "    data['t_sin'] = np.sin(2 * np.pi * data['t'] / 48)\n",
        "    data['t_cos'] = np.cos(2 * np.pi * data['t'] / 48)\n",
        "\n",
        "\n",
        "    # 変換表を作成\n",
        "    day_of_week_mapping = {0: 'Sun', 1: 'Mon', 2: 'Tue', 3: 'Wed', 4: 'Thu', 5: 'Fri', 6: 'Sat'}\n",
        "\n",
        "    # d % 7 の値を文字列に変換\n",
        "    data['day_of_week'] = data['d'] % 7\n",
        "    data['day_of_week'] = data['day_of_week'].map(day_of_week_mapping)\n",
        "\n",
        "    # ダミー変数化して追加\n",
        "    dummies = pd.get_dummies(data['day_of_week'], prefix='day')\n",
        "    data = pd.concat([data, dummies], axis=1)\n",
        "\n",
        "    # 平日フラグを追加\n",
        "    data['weekday_flag'] = 1  # 平日を示す初期値を設定\n",
        "\n",
        "    # 'Sat' と 'Sun' の場合には平日フラグを0に設定\n",
        "    data.loc[data['day_of_week'].isin(['Sat', 'Sun']), 'weekday_flag'] = 0\n",
        "\n",
        "    # 不要な列を削除\n",
        "    # data.drop(['day_of_week'], axis=1, inplace=True)\n",
        "\n",
        "    data['AM_PM'] = data['t'].apply(lambda x: 'AM' if (0 <= x <= 24) else ('PM' if (25 <= x <= 48) else np.nan))\n",
        "\n",
        "    # 活動時間追加\n",
        "    data['activetime'] = data['t'].apply(lambda x: 'act' if (18 <= x <= 22) or (33 <= x <= 37) else ('high_act' if (23 <= x <= 32) else ('rest' if (13 <= x <= 17) or (38 <= x <= 43) else ('deep_rest' if (0 <= x <= 12) or (44 <= x <= 48) else np.nan))))\n",
        "\n",
        "    dummies_3 = pd.get_dummies(data['AM_PM'])\n",
        "    data = pd.concat([data, dummies_3], axis=1)\n",
        "\n",
        "    dummies_4 = pd.get_dummies(data['activetime'])\n",
        "    data = pd.concat([data, dummies_4], axis=1)\n",
        "\n",
        "    print(\"complete basic feature creation\")\n",
        "\n",
        "\n",
        "    cell = pd.read_csv('cell_POIcat.csv')\n",
        "\n",
        "    cell['POIcategory'] = cell['POIcategory'].astype(str)\n",
        "    pivot_df = cell.pivot_table(index=['x', 'y'], columns='POIcategory', values='POI_count', aggfunc='sum').fillna(0).astype(int)\n",
        "    pivot_df.columns = ['POIcategory_' + col for col in pivot_df.columns]\n",
        "\n",
        "    # 元のデータフレームのC列と結合\n",
        "    cell = cell[['x', 'y']].merge(pivot_df, on=['x', 'y'], how='left').drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    # 1. cellのx、yの値から新たに「x_y」のカラムをcellに作成する\n",
        "    cell['x_y'] = cell['x'].astype(str) + '_' + cell['y'].astype(str)\n",
        "\n",
        "\n",
        "    # 2. dataのx、yの値からも新たに「x_y」のカラムをdataに作成する\n",
        "    data['x_y'] = data['x'].astype(str) + '_' + data['y'].astype(str)\n",
        "\n",
        "    # 3. dataを一行ごとの「x_y」にマッチするcellの「x_y」のPOI_catagoryカラムを持ってきてdataに追加する\n",
        "\n",
        "    col=list(cell.filter(like=\"POIcategory_\").columns)\n",
        "    data = data.join(cell.set_index('x_y')[col], on='x_y')\n",
        "\n",
        "    data = data.fillna(0)\n",
        "\n",
        "\n",
        "    l = list(data.filter(like=\"POIcategory_\").columns)\n",
        "    lis = ['uid','activetime']\n",
        "    col=l+lis\n",
        "\n",
        "    temp=data[col]\n",
        "\n",
        "    freq = temp.groupby(['uid', 'activetime']).agg(lambda x: (x != 0).sum()).reset_index()\n",
        "\n",
        "    # カラム名の変更\n",
        "    freq.columns = ['uid'] + ['activetime']+ ['freq_POICategory_' + str(i) for i in range(1, 86)]\n",
        "\n",
        "    # 1から80までのカラムを合計して新しいカラム 'Total_Frequency' を作成\n",
        "    freq['Total_Frequency'] = freq.iloc[:, 1:85].sum(axis=1)\n",
        "\n",
        "    # 1から80までのカラムの値を割合に更新\n",
        "    for i in range(1, 86):\n",
        "        freq[f'freq_POICategory_{i}'] = freq[f'freq_POICategory_{i}'] / freq['Total_Frequency']\n",
        "\n",
        "    # 不要なカラムを削除（Total_Frequency はもう不要なので削除）\n",
        "    freq = freq.drop(columns=['Total_Frequency'])\n",
        "\n",
        "    data = data.merge(freq, on=['uid', 'activetime'], how='left')\n",
        "\n",
        "    l = list(data.filter(like=\"POIcategory_\").columns)\n",
        "    data = data.drop(columns=[col for col in data.columns if col in l])\n",
        "\n",
        "    print(\"complete POI Category frequency feature creation\")\n",
        "\n",
        "\n",
        "\n",
        "    l = list(data.filter(like=\"freq_POICategory_\").columns)\n",
        "\n",
        "    X = data[l]\n",
        "\n",
        "    # K-means クラスタリング\n",
        "    kmeans = KMeans(n_clusters=pcnum, random_state=456)\n",
        "    kmeans.fit(X)\n",
        "    labels = kmeans.labels_\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "    data[\"Cluster\"] = labels\n",
        "\n",
        "    dummies_1 = pd.get_dummies(data['Cluster'], prefix='PCluster')\n",
        "    data = pd.concat([data, dummies_1], axis=1)\n",
        "\n",
        "    l.append('Cluster')\n",
        "    data = data.drop(\"Cluster\",axis=1)\n",
        "\n",
        "\n",
        "\n",
        "#     # 混合ガウス分布モデルを作成\n",
        "#     n_components = pcnum  # クラスタ数を指定\n",
        "#     gmm = GaussianMixture(n_components=n_components, random_state=456)\n",
        "\n",
        "#     # データをモデルに適合\n",
        "#     gmm.fit(X)\n",
        "\n",
        "#     # 各データポイントが各クラスタに所属する確率を計算\n",
        "#     cluster_probs = gmm.predict_proba(X)\n",
        "\n",
        "#     # 結果をデータフレームに追加\n",
        "#     for i in range(n_components):\n",
        "#         data[f'PCluster_{i}'] = cluster_probs[:, i]\n",
        "\n",
        "    # # 結果を表示\n",
        "    # #print(data.filter(like=\"Cluster_\").head())\n",
        "\n",
        "    # data.head().T\n",
        "\n",
        "    # l = list(data.filter(like=\"freq_POICategory_\").columns)\n",
        "    # data = data.drop(columns=[col for col in data.columns if col in l])\n",
        "\n",
        "    print(\"complete cluster feature of POI Category frequency feature creation\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    X = data[['d_sin', 'd_cos', 't_sin', 't_cos', 'AM', 'PM', 'act', 'high_act', 'rest', 'deep_rest']]\n",
        "    print(len(data))\n",
        "\n",
        "    # K-means クラスタリング\n",
        "    kmeans = KMeans(n_clusters=bcnum, random_state=456)\n",
        "    kmeans.fit(X)\n",
        "    labels = kmeans.labels_\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "    # data_ = data.copy()\n",
        "\n",
        "    print(len(labels))\n",
        "    data[\"Cluster\"] = labels\n",
        "\n",
        "    dummies_1 = pd.get_dummies(data['Cluster'], prefix='GCluster')\n",
        "    data = pd.concat([data, dummies_1], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "#     # 混合ガウス分布モデルを作成\n",
        "#     gmm = GaussianMixture(n_components=bcnum, random_state=456)\n",
        "\n",
        "#     # データをモデルに適合\n",
        "#     gmm.fit(X)\n",
        "\n",
        "#     # 各データポイントが各クラスタに所属する確率を計算\n",
        "#     cluster_probs = gmm.predict_proba(X)\n",
        "\n",
        "#     # 結果をデータフレームに追加\n",
        "#     for i in range(bcnum):\n",
        "#         data[f'GCluster_{i}'] = cluster_probs[:, i]\n",
        "\n",
        "    # 結果を表示\n",
        "    #print(data.filter(like=\"Cluster_\").head())\n",
        "\n",
        "    print(\"complete cluster feature of basic feature creation\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def model_xy(uid, data, col_list):\n",
        "\n",
        "    # 目標変数が999以外のデータのみを選択\n",
        "    data = data[(data['uid'] == uid) & (data['x'] != 999) & (data['y'] != 999)]\n",
        "\n",
        "    X = data[col_list]\n",
        "    y_x = data['x']\n",
        "    y_y = data['y']\n",
        "    X.head().T\n",
        "    y_x.head()\n",
        "    y_y.head()\n",
        "\n",
        "    # モデルの構築\n",
        "    model_x = SVR(kernel='rbf')  # RBFカーネルを使用\n",
        "    model_y = SVR(kernel='rbf')  # RBFカーネルを使用\n",
        "\n",
        "    # モデルの学習\n",
        "    model_x.fit(X, y_x)\n",
        "    model_y.fit(X, y_y)\n",
        "\n",
        "    return model_x, model_y\n",
        "\n",
        "print('reading learning function...')\n",
        "\n",
        "\n",
        "# 予測値の関数を作成\n",
        "def predict_xy(features, model_x, model_y):\n",
        "\n",
        "    pred_x = model_x.predict(features)\n",
        "    pred_y = model_y.predict(features)\n",
        "    return pred_x, pred_y\n",
        "\n",
        "\n",
        "print('reading predicting function...')\n",
        "\n",
        "\n",
        "\n",
        "def evaluation(data, reference_data, col_list):\n",
        "\n",
        "    ###精度検証\n",
        "\n",
        "    uids = set(data['uid'])\n",
        "\n",
        "    geobleu_vals = []\n",
        "    dtw_vals = []\n",
        "\n",
        "    for uid in uids:\n",
        "        generated = []\n",
        "        reference = []\n",
        "        print(f\"uid: {uid}\")\n",
        "\n",
        "        # 処理2と処理3: x=999とy=999のところのuidをgeneratedとreferenceに追加\n",
        "        mask = (data['uid'] == uid) & (data['x'] == 999) & (data['y'] == 999)\n",
        "        generated_rows = data[mask]\n",
        "\n",
        "        if len(generated_rows)>0:\n",
        "#             print(f\"uid: {uid}\")\n",
        "            model_x, model_y = model_xy(uid,data,col_list)\n",
        "\n",
        "            for index, row in generated_rows.iterrows():\n",
        "                t = row['t']\n",
        "                d = row['d']\n",
        "                features = row[col_list]\n",
        "                avg_x, avg_y = predict_xy(features.values.reshape(1, -1), model_x, model_y)\n",
        "                # xとyの値を四捨五入して整数値に変換\n",
        "                avg_x = np.where(avg_x < 1, 1, np.where(avg_x > 200, 200, avg_x.round().astype(int)))\n",
        "                avg_y = np.where(avg_y < 1, 1, np.where(avg_y > 200, 200, avg_y.round().astype(int)))\n",
        "\n",
        "                # データを整形してリストに追加\n",
        "                generated.append((d, t, avg_x[0], avg_y[0]))  # avg_xとavg_yを取得する際に[0]を指定して値を取り出す\n",
        "                reference_row = reference_data[(reference_data['uid'] == uid) & (reference_data['d'] == d) & (reference_data['t'] == t)]\n",
        "                if not reference_row.empty:\n",
        "                    reference.append((reference_row['d'].values[0], reference_row['t'].values[0], reference_row['x'].values[0], reference_row['y'].values[0]))\n",
        "\n",
        "        if len(generated_rows) > 0:\n",
        "            # 処理4: 評価指標の計算\n",
        "            geobleu_val = geobleu.calc_geobleu(generated, reference, processes=3)\n",
        "            dtw_val = geobleu.calc_dtw(generated, reference, processes=3)\n",
        "            geobleu_vals.append(geobleu_val)\n",
        "            dtw_vals.append(dtw_val)\n",
        "\n",
        "#             print(\"geobleu:\", geobleu_val)\n",
        "#             print(\"dtw:\", dtw_val)\n",
        "\n",
        "\n",
        "    # 平均と標準偏差を計算して出力\n",
        "    average_geobleu = sum(geobleu_vals) / len(geobleu_vals)\n",
        "    std_geobleu = (sum((x - average_geobleu) ** 2 for x in geobleu_vals) / len(geobleu_vals)) ** 0.5\n",
        "\n",
        "    average_dtw = sum(dtw_vals) / len(dtw_vals)\n",
        "    std_dtw = (sum((x - average_dtw) ** 2 for x in dtw_vals) / len(dtw_vals)) ** 0.5\n",
        "\n",
        "    print(\"Average geobleu:\", average_geobleu)\n",
        "    print(\"Standard deviation geobleu:\", std_geobleu)\n",
        "    print(\"Average dtw:\", average_dtw)\n",
        "    print(\"Standard deviation dtw:\", std_dtw)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('<your path>')\n",
        "reference_data = pd.read_csv('<your path>')\n",
        "data = featurecreation(data=data, pcnum=5, bcnum=5)\n",
        "\n",
        "freq_col = list(data.filter(like=\"freq_POICategory_\").columns)\n",
        "freq_cluster_col = list(data.filter(like=\"PCluster_\").columns)\n",
        "basic_cluster_col = list(data.filter(like=\"GCluster_\").columns)\n",
        "basic_col = ['d_sin', 'd_cos', 't_sin', 't_cos',\n",
        "          'day_Sun', 'day_Mon', 'day_Tue', 'day_Wed', 'day_Thu', 'day_Fri', 'day_Sat', 'weekday_flag', 'AM', 'PM', 'act', 'high_act', 'rest', 'deep_rest']\n",
        "\n",
        "col_list = basic_col+basic_cluster_col+freq_cluster_col\n",
        "evaluation(data,reference_data, col_list)"
      ],
      "metadata": {
        "id": "o8dASmd40Y3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FAiPvccR0Y9Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}